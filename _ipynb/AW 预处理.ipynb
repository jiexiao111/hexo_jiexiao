{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#setup\" data-toc-modified-id=\"setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>setup</a></span></li><li><span><a href=\"#测试简单函数\" data-toc-modified-id=\"测试简单函数-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>测试简单函数</a></span><ul class=\"toc-item\"><li><span><a href=\"#翻译\" data-toc-modified-id=\"翻译-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>翻译</a></span></li><li><span><a href=\"#删除-%-%-[]-内的内容\" data-toc-modified-id=\"删除-%-%-[]-内的内容-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>删除 % % [] 内的内容</a></span></li><li><span><a href=\"#测试-spacy\" data-toc-modified-id=\"测试-spacy-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>测试 spacy</a></span></li><li><span><a href=\"#nltk\" data-toc-modified-id=\"nltk-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>nltk</a></span></li><li><span><a href=\"#生成训练数据\" data-toc-modified-id=\"生成训练数据-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>生成训练数据</a></span></li><li><span><a href=\"#近似向量查找\" data-toc-modified-id=\"近似向量查找-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>近似向量查找</a></span></li></ul></li><li><span><a href=\"#调试函数\" data-toc-modified-id=\"调试函数-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>调试函数</a></span><ul class=\"toc-item\"><li><span><a href=\"#集成测试\" data-toc-modified-id=\"集成测试-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>集成测试</a></span></li><li><span><a href=\"#单元测试\" data-toc-modified-id=\"单元测试-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>单元测试</a></span></li></ul></li><li><span><a href=\"#收集专有词汇\" data-toc-modified-id=\"收集专有词汇-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>收集专有词汇</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -r 1-17 G\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from stemming.porter2 import stem\n",
    "from googletrans import Translator\n",
    "from multiprocessing import Pool\n",
    "\n",
    "try:\n",
    "    from ..nmt_common_fn import *\n",
    "except:\n",
    "    sys.path.append('/aiml/data/DevMind/06.SRC/01.TestCaseBot/02.TestAWBot/01.NMT_Train')\n",
    "    from nmt.nmt_common_fn import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试简单函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Configure basic services of Layer 2 multicast instances'"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "translator.translate(\"去配置二层组播实例基础业务\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除 % % [] 内的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'display pim '"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "reg_bracket = re.compile('\\[.*?\\]|\\%.*?\\%')\n",
    "reg_bracket.sub(' ', 'display pim[ %ver%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试 spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5829033582528316"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install: pip install spacy && python -m spacy download en\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Process whole documents\n",
    "text = u'the fries were gross'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "\n",
    "# Determine semantic similarities\n",
    "doc1 = nlp(u'the fries were gross')\n",
    "doc2 = nlp(u'worst fries ever')\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'snoop'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'snooping'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize('dogs')\n",
    "wordnet_lemmatizer.lemmatize('snooping', pos='v')\n",
    "wordnet_lemmatizer.lemmatize('snooping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词干提取，参考[http://www.zmonster.me/2016/01/21/lemmatization-survey.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'cri'"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmer.stem('dogs')\n",
    "porter_stemmer.stem('crying')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/tmp/clean_data.csv')\n",
    "with open('/tmp/clean_data.txt', 'w') as f:\n",
    "    f.writelines([x + '\\n' for x in df['train_data'].tolist() if isinstance(x, str)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 近似向量查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s Deal_Str,deal_with_pd,read_all_csv /aiml/data/DevMind/06.SRC/01.TestCaseBot/02.TestAWBot/01.NMT_Train/nmt/preprocess/clean_data.py\n",
    "class Deal_Str(object):\n",
    "    def __init__(self):\n",
    "        self.translator = Translator()\n",
    "        self.regx = re.compile('\\[.*?\\]|\\%.*?\\%')\n",
    "        # wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        if os.path.exists('wordninja_words.txt'):\n",
    "            jieba.load_userdict('wordninja_words.txt')\n",
    "        elif os.path.exists('./nmt/preprocess/wordninja_words.txt'):\n",
    "            jieba.load_userdict('./nmt/preprocess/wordninja_words.txt')\n",
    "        else:\n",
    "            jieba.load_userdict('/aiml/data/DevMind/06.SRC/01.TestCaseBot/02.TestAWBot/01.NMT_Train/nmt/preprocess/wordninja_words.txt')\n",
    "        print(\"Load Jieba Dict\")\n",
    "\n",
    "    @staticmethod\n",
    "    @decorator_trace_call\n",
    "    def method_to_des(mth):\n",
    "        \"\"\"\n",
    "        将给定的字符串依据驼峰命名/下划线进行切分\n",
    "        \"\"\"\n",
    "\n",
    "        mth = re.sub(\"[^A-Za-z0-9?=]\", \" \", mth)\n",
    "        if not mth.islower():\n",
    "            mth_arr = []\n",
    "            for word in mth.split(\" \"):\n",
    "                if word.islower():\n",
    "                    mth_arr.append(word)\n",
    "                    continue\n",
    "\n",
    "                tmp_word = \"\"\n",
    "                compare_len = len(word) - 1\n",
    "                for i, alp in enumerate(word):\n",
    "                    tmp_word += alp\n",
    "                    if i == compare_len:\n",
    "                        mth_arr.append(tmp_word)\n",
    "                        break\n",
    "                    before_alp_lower = word[i].islower()\n",
    "                    after_alp_upper = word[i + 1].isupper()\n",
    "                    before_alp_digit = word[i].isdigit()\n",
    "                    if (i>0 and before_alp_digit and word[i-1].isupper() != after_alp_upper) \\\n",
    "                        or (before_alp_lower and after_alp_upper):\n",
    "                        mth_arr.append(tmp_word)\n",
    "                        tmp_word = \"\"\n",
    "            mth_result = ' '.join(mth_arr)\n",
    "        else:\n",
    "            mth_result = mth\n",
    "        mth_result = ' '.join([x.strip().lower() for x in mth_result.split() if \"\" != x])\n",
    "        return mth_result\n",
    "\n",
    "    @staticmethod\n",
    "    @decorator_trace_call\n",
    "    def rm_new_line_toke(input_str):\n",
    "        \"\"\"\n",
    "        删除多余的符号\n",
    "        \"\"\"\n",
    "\n",
    "        return ' '.join([x for x in input_str.split('\\\\r\\\\n') if x != ''])\n",
    "\n",
    "    @decorator_trace_call\n",
    "    def recreate_session(self):\n",
    "        self.translator = Translator()\n",
    "\n",
    "    @decorator_trace_call\n",
    "    def deal_english_like_str(self, input_str):\n",
    "        \"\"\"\n",
    "        处理类似英文的输入\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(input_str, str):\n",
    "            # 删除 % % [] 内的内容\n",
    "            ret_str = self.regx.sub(' ', input_str)\n",
    "            ret_str = ' '.join([x.strip() for x in ret_str.split() if \"\" != x])\n",
    "            # 删除 \\r\\n 然后根据函数命名的特征完成分词\n",
    "            ret_str = method_to_des(rm_new_line_toke(ret_str))\n",
    "            # 完成词形还原 词干提取\n",
    "            # ret_str = ' '.join([wordnet_lemmatizer.lemmatize(x) for x in ret_str.split()])\n",
    "            # 通过jieba将连续的包含多个词的字串分开\n",
    "            ret_str = ' '.join([x for x in jieba.cut(ret_str) if \"\" != x and not x.isspace()])\n",
    "            return ret_str\n",
    "        else:\n",
    "            return input_str\n",
    "\n",
    "    @decorator_trace_call\n",
    "    def deal_chinese_like_str(self, input_str):\n",
    "        \"\"\"\n",
    "        处理含义中文的输入\n",
    "        \"\"\"\n",
    "\n",
    "        def has_chinese(input_str):\n",
    "            for x in input_str:\n",
    "                if ord(x) <= 0x9FA5 and ord(x) >= 0x4E00:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        if isinstance(input_str, str) and has_chinese(input_str):\n",
    "            # 删除 \\r\\n\n",
    "            ret_str = rm_new_line_toke(input_str)\n",
    "            # 通过翻译接口将中文转换为英文\n",
    "            ret_str = self.translator.translate(ret_str).text\n",
    "            return ret_str\n",
    "        else:\n",
    "            return input_str\n",
    "\n",
    "def deal_with_pd(df_idx):\n",
    "    df = df_idx[0]\n",
    "    ret_idx = df_idx[1]\n",
    "    deal_str = Deal_Str()\n",
    "    new_aw_des = []\n",
    "    new_attr_aw_keyword = []\n",
    "    new_aw_cmd = []\n",
    "    new_attr_belongingclass = []\n",
    "    new_attr_aw = []\n",
    "    train_data = []\n",
    "    process_idx = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        process_idx += 1\n",
    "        if process_idx % 500 == 0:\n",
    "            deal_str.recreate_session()\n",
    "        aw_des = deal_str.deal_english_like_str(deal_str.deal_chinese_like_str(row['aw_des']))\n",
    "        attr_aw_keyword = deal_str.deal_english_like_str(row['attr_aw_keyword'])\n",
    "        aw_cmd = deal_str.deal_english_like_str(row['aw_cmd'])\n",
    "        attr_belongingclass = deal_str.deal_english_like_str(row['attr_belongingclass'])\n",
    "        attr_aw = deal_str.deal_english_like_str(row['attr_aw'])\n",
    "        single_train_data = ' '.join([x for x in [aw_des, attr_aw_keyword, aw_cmd, attr_belongingclass, attr_aw] if isinstance(x, str)])\n",
    "\n",
    "        new_aw_des.append(aw_des)\n",
    "        new_attr_aw_keyword.append(attr_aw_keyword)\n",
    "        new_aw_cmd.append(aw_cmd)\n",
    "        new_attr_belongingclass.append(attr_belongingclass)\n",
    "        new_attr_aw.append(attr_aw)\n",
    "        train_data.append(single_train_data)\n",
    "\n",
    "    df = df.assign(new_aw_des=new_aw_des)\n",
    "    df = df.assign(new_attr_aw_keyword=new_attr_aw_keyword)\n",
    "    df = df.assign(new_aw_cmd=new_aw_cmd)\n",
    "    df = df.assign(new_attr_belongingclass=new_attr_belongingclass)\n",
    "    df = df.assign(new_attr_aw=new_attr_aw)\n",
    "    df = df.assign(train_data=train_data)\n",
    "    df = df.loc[:, ['_id', 'aw_des', 'new_aw_des', 'attr_aw_keyword', 'new_attr_aw_keyword',\n",
    "                    'aw_cmd', 'new_aw_cmd', 'attr_belongingclass', 'new_attr_belongingclass',\n",
    "                    'attr_aw', 'new_attr_aw', 'train_data']]\n",
    "    return ret_idx, df\n",
    "\n",
    "@decorator_trace_call\n",
    "def read_all_csv(csv_list):\n",
    "    \"\"\"\n",
    "    读取多个 CSV 文件，选取需要的字段然后合并\n",
    "    \"\"\"\n",
    "\n",
    "    tmp_df_list = []\n",
    "    for csv in csv_list:\n",
    "        if os.path.exists(csv):\n",
    "            tmp_df = pd.read_csv(csv)\n",
    "            tmp_df = tmp_df.loc[:, ['_id', 'aw_des', 'attr_aw_keyword', 'aw_cmd', 'attr_belongingclass', 'attr_aw']]\n",
    "            tmp_df_list.append(tmp_df)\n",
    "            print(\"Read CSV file: %s\" % (csv))\n",
    "        else:\n",
    "            print(\"Cannot find CSV file: %s\" % (csv))\n",
    "    return pd.concat(tmp_df_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read CSV file: /aiml/data/NoS_AW_Script/aw_baw.csv\n",
      "Read CSV file: /aiml/data/NoS_AW_Script/aw_caw.csv\n",
      "Load Jieba Dict\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# %load -r 170- /aiml/data/DevMind/06.SRC/01.TestCaseBot/02.TestAWBot/01.NMT_Train/nmt/preprocess/clean_data.py\n",
    "set_debug_flag()\n",
    "df = read_all_csv([\"/aiml/data/NoS_AW_Script/aw_baw.csv\", \"/aiml/data/NoS_AW_Script/aw_caw.csv\"])\n",
    "process_num = 40\n",
    "line_per_process = 200\n",
    "\n",
    "# TODO 调试代码\n",
    "df = df[100: 200]\n",
    "\n",
    "df_list = []\n",
    "for idx in range(0, len(df), line_per_process):\n",
    "    df_list.append([df[idx: idx+line_per_process], idx])\n",
    "\n",
    "pool = Pool(process_num)\n",
    "df_list = pool.map(deal_with_pd, df_list)\n",
    "df_sorted = [x[1] for x in sorted(df_list, key=lambda x: x[0])]\n",
    "pd.concat(df_sorted).to_csv('/tmp/first_tmp2.csv')\n",
    "print(all(pd.concat(df_sorted)['_id'] == df['_id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat(df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jieba Dict\n"
     ]
    }
   ],
   "source": [
    "deal_str = Deal_Str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_str = '去配置二层组播实例基础业务'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Configure basic services of Layer 2 multicast instances'"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal_str.deal_chinese_like_str(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check_igmpsnooping_portinfo\"\\r\\n'"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_str = deal_str.regx.sub(' ', input_str);ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check_igmpsnooping_portinfo\"'"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_str = ' '.join([x.strip() for x in ret_str.split() if \"\" != x]);ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check igmpsnooping portinfo'"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_str = method_to_des(rm_new_line_toke(ret_str));ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check igmpsnooping portinfo'"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_str = ' '.join([wordnet_lemmatizer.lemmatize(x) for x in ret_str.split()]);ret_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = 'm'\n",
    "tmp.isspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 收集专有词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/tmp/filed_word_origin2.csv', 'r') as f:\n",
    "    tmp = set(deal_str.deal_english_like_str(f.read()).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/aiml/data/DevMind/06.SRC/01.TestCaseBot/02.TestAWBot/01.NMT_Train/nmt/preprocess/wordninja_words.txt', 'r') as f:\n",
    "    dict_org = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_1 = [x for x in tmp if x not in dict_org and len(x) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_2 = [x for x in tmp if x not in dict_org and len(x) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_1.extend(dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = 'S'\n",
    "tmp.lower()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {
    "height": "964px",
    "left": "0px",
    "right": "1627px",
    "top": "50px",
    "width": "231px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
