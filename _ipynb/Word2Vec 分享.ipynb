{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文档进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent1 = 'this is a foo bar sentence'.split()\n",
    "sent2 = 'this is another foo bar sentence'.split()\n",
    "texts = [sent1, sent2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据词频为每个单词指定 ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'),\n",
       " (1, 'bar'),\n",
       " (2, 'foo'),\n",
       " (3, 'is'),\n",
       " (4, 'sentence'),\n",
       " (5, 'this'),\n",
       " (6, 'another')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Dictionary(texts)\n",
    "list(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将每个单词的ID转换为onehot形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', array([1., 0., 0., 0., 0., 0., 0.])),\n",
       " ('bar', array([0., 1., 0., 0., 0., 0., 0.])),\n",
       " ('foo', array([0., 0., 1., 0., 0., 0., 0.])),\n",
       " ('is', array([0., 0., 0., 1., 0., 0., 0.])),\n",
       " ('sentence', array([0., 0., 0., 0., 1., 0., 0.])),\n",
       " ('this', array([0., 0., 0., 0., 0., 1., 0.])),\n",
       " ('another', array([0., 0., 0., 0., 0., 0., 1.]))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_onehot(dict_len, word_id):\n",
    "    onehot = np.zeros(dict_len)\n",
    "    onehot[word_id] = 1\n",
    "    return onehot\n",
    "[(word, to_onehot(len(vocab.items()), idx)) for idx, word in vocab.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 缺点 **\n",
    "* one-hot的表示方法是一种稀疏表示方式，当词典非常大时会造成维数灾难，非常高维的向量计算非常困难\n",
    "* one-hot表示方法表示的两个词的词向量是孤立的，不能从两个词的向量中看出两个词之间的语义关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * ** word embedding 是什么？ **\n",
    "    * embedding 是个拓扑学的词。地图就是对于现实地理的embedding，现实的地理地形的信息其实远远超过三维，但是地图通过颜色和等高线等来最大化表现现实的地理信息。\n",
    "    * 计算机在处理自然语言时，无法直接处理单词，所以需要用固定的维度来最大化表现词的信息，也就是所谓的 word embedding，那么 onehot 也是一种 word embedding 的方式\n",
    "    \n",
    "* ** Distributed representation 要解决的问题? **\n",
    "    * 增加特征信息，onehot 解决了词的表示，但是没能最大化表现词的信息，典型的如，语义近似、单复数、时态信息、类比性质（这个性质非常重要，被用来衡量词向量训练结果的好坏）都被丢弃了\n",
    "    * 降低维度\n",
    "\n",
    "* ** word2vec 是什么？ **\n",
    "    * 一种 word embedding 的方法\n",
    "    * 谷歌开源的工具，其中实现了 CBOW(Continuous Bagof-Words) 和 Skip-gram 模型，那就意味着还有很多其他的模型\n",
    "    \n",
    "* ** word2vec 的基本假设 **\n",
    "    * 分布式假设(distributional hypothesis)，出现在相同上下文(context)下的词意思应该相近。所有学习word embedding的方法都是在用数学的方法建模词和context之间的关系。例如下面两个例子，京巴和泰迪的语义相近：\n",
    "        * 这个可爱的 ** 泰迪 ** 舔了我的脸。\n",
    "        * 这个可爱的 ** 京巴 ** 舔了我的脸。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec 算法简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cbow](./CBOW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中 $\\omega _{t-c}$ 就是我们的 $x$，也就是 $word2vec$ 的训练目标，CBOW 的思路是用上下文推测当前单词，例如：** 我 是 华为 的 软件开发 人员 ** 那么我们会得到下面几个训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('我', '是', '的', '软件开发'), '华为'],\n",
      " [('是', '华为', '软件开发', '人员'), '的']]\n"
     ]
    }
   ],
   "source": [
    "text = \"我 是 华为 的 软件开发 人员\"\n",
    "word_list = text.split()\n",
    "train_sample = [[(word_list[c-2], word_list[c-1], word_list[c+1], word_list[c+2]), word_list[c]] for c in range(2, len(word_list)-2)]\n",
    "pprint.pprint(train_sample, width=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将词初始化为固定长度的向量，向量的值是随机的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'人员': [5, 5, 5, 5, 5],\n",
       " '华为': [2, 2, 2, 2, 2],\n",
       " '我': [0, 0, 0, 0, 0],\n",
       " '是': [1, 1, 1, 1, 1],\n",
       " '的': [3, 3, 3, 3, 3],\n",
       " '软件开发': [4, 4, 4, 4, 4]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordembedding = {v:[i]*5 for i, v in enumerate(word_list)}\n",
    "wordembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练数据转为初始化的 word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('我', '是', '的', '软件开发'),\n",
       " [[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [3, 3, 3, 3, 3], [4, 4, 4, 4, 4]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('华为', [2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y = train_sample[0]\n",
    "train_x, [wordembedding[x] for x in train_x]\n",
    "train_y, wordembedding[train_y] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将$x$传入投影层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 8, 8, 8])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([wordembedding[x] for x in train_x], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里就很熟悉了，我们通过样本 $x=[8, 8, 8, 8, 8]$ $y=[2, 2, 2, 2]$ 去训练一个模型。\n",
    "$ML$算法大体可以理解为\n",
    "$$ y=wx+b $$\n",
    "其中 $ y $ 是输出， $ x $ 是输入， $ w $ 和 $ b $ 是模型中的参数。我们向模型传入大量的训练数据，即 $(x, y)$，然后，通过反向传播和梯度下降，不断优化 $w$ 和 $b$，但是在 $word2vec$ 中，我们需要优化的是 $x$ 所以，在迭代过程中，我们需要将 $w$ 和 $b$ 视为固定值，然后通过反向传播和梯度下降去优化 $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 $CBOW$ 中 $x$ 是多个词的简单加和，所以，会将梯度下降的程度平均返回到各个 $word vector$ 中，如 $x$ 由 $[8, 8, 8, 8, 8]$ 优化到 $[7, 7, 7, 7, 7]$，则每个 $word vector$ 都会减小 $0.25$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ![skip-gram](./skip-gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 CBOW 的区别\n",
    "* 输入和输出相反，所以输入只有一个vector，不需要做加和了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Softmax 与 Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种提升 $wordembedding$ 的算法，其中 $Negative Sampling$ 性能更好。$Hierarchical Softmax$ 相较于传统的 $Softmax$ 性能已经有大幅提升，时间复杂度由 $O(N)$ 降低到 $O(Log_{2}(N))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentences/paragraphs/documents Representation 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bag of words\n",
    "    * 有如下缺点：1.没有考虑到单词的顺序，2.忽略了单词的语义信息。因此这种方法对于短文本效果很差，对于长文本效果一般\n",
    "* LDA\n",
    "    * 计算出一片文档或者句子的主题分布\n",
    "* average word vectors\n",
    "    * 简单的对句子中的所有词向量取平均。是一种简单有效的方法，但缺点也是没有考虑到单词的顺序\n",
    "* tfidf-weighting word vectors\n",
    "    * 对句子中的所有词向量根据tfidf权重加权求和，是常用的一种计算sentence embedding的方法，在某些问题上表现很好，相比于简单的对所有词向量求平均，考虑到了tfidf权重，因此句子中更重要的词占得比重就更大。但缺点也是没有考虑到单词的顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec 算法简介\n",
    "首先，Doc2Vec 能做什么，与 word2vec 一样，Doc2Vec 只能将他见过的 doc 转换为 vector，即，只能转换训练集中出现过的 doc，那么 doc2vec 将不适用于 doc 搜索任务，因为用户新输入的 doc 无法转换成 vector，然后再去已有的数据库中查找相似文档，但是 doc2vec 可以对训练集中的 doc 进行转换，抽取出语义级别的信息，然后根据语义近似的信息对 doc 进行分类，典型的，如，情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DM (distributed memory)\n",
    "![DM](./DM.png)\n",
    "类似于 CBOW。DM模型在训练时，首先将每个文档ID和语料库中的所有词初始化一个K维的向量，然后将文档向量和上下文词的向量输入模型，隐层将这些向量累加（或取均值、或直接拼接起来）得到中间向量，作为输出层softmax的输入。在一个文档的训练过程中，文档ID保持不变，共享着同一个文档向量，相当于在预测单词的概率时，都利用了真个句子的语义。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBOW (distributed bag of words)\n",
    "![DBOW](./DBOW.png)\n",
    "类似于Skip-gram，DBOW模型的输入是文档的向量，预测的是该文档中随机抽样的词。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
