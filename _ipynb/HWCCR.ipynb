{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#读取-HWDB-数据\" data-toc-modified-id=\"读取-HWDB-数据-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>读取 HWDB 数据</a></span><ul class=\"toc-item\"><li><span><a href=\"#真实训练数据\" data-toc-modified-id=\"真实训练数据-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>真实训练数据</a></span></li><li><span><a href=\"#测试数据-MNIST\" data-toc-modified-id=\"测试数据-MNIST-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>测试数据 MNIST</a></span><ul class=\"toc-item\"><li><span><a href=\"#将-gz-格式的-mnist-数据转换为-png-格式\" data-toc-modified-id=\"将-gz-格式的-mnist-数据转换为-png-格式-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>将 gz 格式的 mnist 数据转换为 png 格式</a></span></li></ul></li></ul></li><li><span><a href=\"#数据预处理\" data-toc-modified-id=\"数据预处理-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>数据预处理</a></span></li><li><span><a href=\"#模型定义与初始化\" data-toc-modified-id=\"模型定义与初始化-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>模型定义与初始化</a></span><ul class=\"toc-item\"><li><span><a href=\"#SigleDense_Custom\" data-toc-modified-id=\"SigleDense_Custom-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>SigleDense_Custom</a></span></li><li><span><a href=\"#ResNet_Custom\" data-toc-modified-id=\"ResNet_Custom-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>ResNet_Custom</a></span><ul class=\"toc-item\"><li><span><a href=\"#HCCR_Resnet-项目分析\" data-toc-modified-id=\"HCCR_Resnet-项目分析-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>HCCR_Resnet 项目分析</a></span></li><li><span><a href=\"#ResNet-的理解及其-Keras-实现\" data-toc-modified-id=\"ResNet-的理解及其-Keras-实现-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>ResNet 的理解及其 Keras 实现</a></span></li><li><span><a href=\"#ResNet-TensorFlow-实现参考\" data-toc-modified-id=\"ResNet-TensorFlow-实现参考-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>ResNet TensorFlow 实现参考</a></span></li><li><span><a href=\"#在-jupter-中-import-函数\" data-toc-modified-id=\"在-jupter-中-import-函数-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>在 jupter 中 import 函数</a></span></li><li><span><a href=\"#ResNet18\" data-toc-modified-id=\"ResNet18-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>ResNet18</a></span></li><li><span><a href=\"#ResNet34\" data-toc-modified-id=\"ResNet34-4.2.6\"><span class=\"toc-item-num\">4.2.6&nbsp;&nbsp;</span>ResNet34</a></span></li><li><span><a href=\"#ResNet50\" data-toc-modified-id=\"ResNet50-4.2.7\"><span class=\"toc-item-num\">4.2.7&nbsp;&nbsp;</span>ResNet50</a></span></li><li><span><a href=\"#ResNet101\" data-toc-modified-id=\"ResNet101-4.2.8\"><span class=\"toc-item-num\">4.2.8&nbsp;&nbsp;</span>ResNet101</a></span></li><li><span><a href=\"#ResNet152\" data-toc-modified-id=\"ResNet152-4.2.9\"><span class=\"toc-item-num\">4.2.9&nbsp;&nbsp;</span>ResNet152</a></span></li></ul></li><li><span><a href=\"#Xception_Keras\" data-toc-modified-id=\"Xception_Keras-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Xception_Keras</a></span></li></ul></li><li><span><a href=\"#训练\" data-toc-modified-id=\"训练-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>训练</a></span><ul class=\"toc-item\"><li><span><a href=\"#sigle_dense_v.01\" data-toc-modified-id=\"sigle_dense_v.01-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>sigle_dense_v.01</a></span></li><li><span><a href=\"#sigle_dense_v.02\" data-toc-modified-id=\"sigle_dense_v.02-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>sigle_dense_v.02</a></span></li><li><span><a href=\"#sigle_dense_v.03-——-TODO\" data-toc-modified-id=\"sigle_dense_v.03-——-TODO-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>sigle_dense_v.03 —— TODO</a></span></li><li><span><a href=\"#ResNet_v0.1\" data-toc-modified-id=\"ResNet_v0.1-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>ResNet_v0.1</a></span></li><li><span><a href=\"#ResNet_v.02\" data-toc-modified-id=\"ResNet_v.02-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>ResNet_v.02</a></span></li><li><span><a href=\"#ResNet_v.03\" data-toc-modified-id=\"ResNet_v.03-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>ResNet_v.03</a></span></li></ul></li><li><span><a href=\"#调参\" data-toc-modified-id=\"调参-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>调参</a></span><ul class=\"toc-item\"><li><span><a href=\"#ResNet\" data-toc-modified-id=\"ResNet-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>ResNet</a></span><ul class=\"toc-item\"><li><span><a href=\"#优化函数和学习率-——-TODO\" data-toc-modified-id=\"优化函数和学习率-——-TODO-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>优化函数和学习率 —— TODO</a></span></li><li><span><a href=\"#图像预处理--——-TODO\" data-toc-modified-id=\"图像预处理--——-TODO-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>图像预处理  —— TODO</a></span></li><li><span><a href=\"#其他尝试--——-TODO\" data-toc-modified-id=\"其他尝试--——-TODO-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>其他尝试  —— TODO</a></span></li><li><span><a href=\"#探索学习率衰减规则\" data-toc-modified-id=\"探索学习率衰减规则-6.1.4\"><span class=\"toc-item-num\">6.1.4&nbsp;&nbsp;</span>探索学习率衰减规则</a></span></li><li><span><a href=\"#绘制Res50结构图\" data-toc-modified-id=\"绘制Res50结构图-6.1.5\"><span class=\"toc-item-num\">6.1.5&nbsp;&nbsp;</span>绘制Res50结构图</a></span><ul class=\"toc-item\"><li><span><a href=\"#这个是自己实现的-Resnet50\" data-toc-modified-id=\"这个是自己实现的-Resnet50-6.1.5.1\"><span class=\"toc-item-num\">6.1.5.1&nbsp;&nbsp;</span>这个是自己实现的 Resnet50</a></span></li><li><span><a href=\"#Keras-官方实现的Resnet\" data-toc-modified-id=\"Keras-官方实现的Resnet-6.1.5.2\"><span class=\"toc-item-num\">6.1.5.2&nbsp;&nbsp;</span>Keras 官方实现的Resnet</a></span></li><li><span><a href=\"#对比--Resnet18-和-Resnet20\" data-toc-modified-id=\"对比--Resnet18-和-Resnet20-6.1.5.3\"><span class=\"toc-item-num\">6.1.5.3&nbsp;&nbsp;</span>对比  Resnet18 和 Resnet20</a></span></li></ul></li></ul></li><li><span><a href=\"#打印model中间层输出\" data-toc-modified-id=\"打印model中间层输出-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>打印model中间层输出</a></span></li><li><span><a href=\"#重新读取gnt文件，预处理\" data-toc-modified-id=\"重新读取gnt文件，预处理-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>重新读取gnt文件，预处理</a></span></li><li><span><a href=\"#调用-C++-库\" data-toc-modified-id=\"调用-C++-库-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>调用 C++ 库</a></span></li><li><span><a href=\"#对比三种-Resize-的区别\" data-toc-modified-id=\"对比三种-Resize-的区别-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>对比三种 Resize 的区别</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras提供的-Resize\" data-toc-modified-id=\"Keras提供的-Resize-6.5.1\"><span class=\"toc-item-num\">6.5.1&nbsp;&nbsp;</span>Keras提供的 Resize</a></span></li><li><span><a href=\"#C++-实现版本\" data-toc-modified-id=\"C++-实现版本-6.5.2\"><span class=\"toc-item-num\">6.5.2&nbsp;&nbsp;</span>C++ 实现版本</a></span></li><li><span><a href=\"#python实现版本\" data-toc-modified-id=\"python实现版本-6.5.3\"><span class=\"toc-item-num\">6.5.3&nbsp;&nbsp;</span>python实现版本</a></span></li></ul></li><li><span><a href=\"#特征标准化——TODO\" data-toc-modified-id=\"特征标准化——TODO-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>特征标准化——TODO</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras.optimizers import Nadam, Adam, SGD\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from python_code.ResNet_Keras import ResnetBuilder\n",
    "from keras.utils import plot_model\n",
    "from python_code.resnet_20 import build_resnet\n",
    "from skimage import exposure\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# 控制 Res18 每 10 分钟验证一次\n",
    "step_per_epoch = 512 * 1000 / batch_size \n",
    "# 验证仅使用五分之一的步数，刚好符合训练集和测试集的比例 8:2\n",
    "step_per_valid = step_per_epoch / 5\n",
    "# 按照每个 epoch 10 分钟计算，1000 个 epoch 刚好跑一周，按照比赛的情况，这应该是最大值了\n",
    "max_epoch = 512 * 1000 / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印narray的便利函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plots(ims, interp=False, titles=None):\n",
    "    ims=np.array(ims)\n",
    "    mn,mx=ims.min(),ims.max()\n",
    "    f = plt.figure(figsize=(12,24))\n",
    "    for i in range(len(ims)):\n",
    "        sp=f.add_subplot(1, len(ims), i+1)\n",
    "        if not titles is None: sp.set_title(titles[i], fontsize=18)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none', vmin=mn,vmax=mx)\n",
    "\n",
    "def plot(im, interp=False):\n",
    "    f = plt.figure(figsize=(3,6), frameon=True)\n",
    "    plt.imshow(im, interpolation=None if interp else 'none')\n",
    "\n",
    "plt.gray()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取 HWDB 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 真实训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = '/aiml/data/train/'\n",
    "test_dir = '/aiml/data/test/'\n",
    "log_dir = '/aiml/dfs/checkpoint/train'\n",
    "pix = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试数据 MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要用于确定数据输入正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = '/aiml/data/mnist_train'\n",
    "test_dir = '/aiml/data/mnist_test'\n",
    "log_dir = '/aiml/dfs/checkpoint/test/'\n",
    "pix = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 将 gz 格式的 mnist 数据转换为 png 格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "下载并解压 mnist 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 获取脚本 https://github.com/myleott/mnist_png\n",
    "!wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "!wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "!zcat train-images-idx3-ubyte.gz > train-images-idx3-ubyte\n",
    "!zcat train-labels-idx1-ubyte.gz > train-labels-idx1-ubyte\n",
    "!pip install pypng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "解压有问题，所以需要将训练集分成两部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for num in [str(x) for x in range(10)]:\n",
    "    train_dir = os.path.join('training', num)\n",
    "    test_dir = os.path.join('testing', num)\n",
    "    train_files = [os.path.join(train_dir, x) for x in os.listdir(train_dir)]\n",
    "    test_files = [os.path.join(test_dir, x) for x in os.listdir(test_dir)]\n",
    "    train_files.sort()\n",
    "    test_files.sort()\n",
    "    [os.remove(x) for x in train_files[:500]]\n",
    "    [os.remove(x) for x in test_files[500:]]\n",
    "!find . -name \"*.png\"|cut -d '/' -f 4 |sort |uniq |wc -l\n",
    "!find . -name \"*.png\"| wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data = image.ImageDataGenerator()\n",
    "test_data = image.ImageDataGenerator()\n",
    "train_gen = train_data.flow_from_directory(train_dir, color_mode='grayscale', target_size=(pix, pix), batch_size=batch_size)\n",
    "test_gen = train_data.flow_from_directory(test_dir, color_mode='grayscale', target_size=(pix, pix), batch_size=batch_size)\n",
    "# 类别数量\n",
    "if hasattr(train_gen, 'num_classes'):\n",
    "    num_class = train_gen.num_classes\n",
    "else:\n",
    "    num_class = train_gen.num_class\n",
    "# 训练样本数量\n",
    "num_train = train_gen.n\n",
    "# 测试样本数量\n",
    "num_test = test_gen.n\n",
    "# 确认读取的图片的格式正确\n",
    "image_shape = train_gen.next()[0].shape\n",
    "# 查看返回的标签类型，“categorical” 表示为 one-hot 标签\n",
    "print(train_gen.class_mode)\n",
    "print(num_class, num_train, num_test, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义与初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### SigleDense_Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "主要用于测试数据输入正确性，发现loss俨然有上升趋势，放弃其他尝试，换上 MNIST 后，发现一切正常，说明 HCCR 需要的模型较为复杂，收敛时间更长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'sigle_dense'\n",
    "\n",
    "def sigle_dense_model():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(pix, pix, 1)),\n",
    "        Dense(512, activation='softmax'),\n",
    "        Dense(num_class, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = sigle_dense_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet_Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### HCCR_Resnet 项目分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Github上有个 caffe 的 [Demo](https://github.com/tianrolin/HCCR-ResNet)，号称能达到 97% 的准确率，虽然没搞过 caffe，还是强行分析一波。在 Readme 中提到两点：\n",
    "* 注意到这里的每张字符图像尺寸不同，所以需要进行resize预处理，这里将其尺寸统一resize为56x56，再在图像上下左右各补充4个像素的白边，最终补成64x64的图像。\n",
    "* 由于原图像灰度区间较窄，因此在训练网络前对图像做对比度增强，将每张图像的灰度拉伸到0~255，有助于识别效果提升。\n",
    "\n",
    "第一点，后续确认一下，貌似我直接就是按 64 * 64 读取的图像。\n",
    "第二点，需要找到对应接口进行实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "输入层，可以看出用了 1./255 的 scale，batch_size 选择了 100，我们的 K80 有更大的显存，后期考虑增加 batch_size 尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer {\n",
    "  type: \"Data\" //data层  \n",
    "  transform_param {\n",
    "    scale: 0.00390625 // 对所有的图片归一化到0~1之间，也就是对输入数据全部乘以scale，0.0039= 1/255  \n",
    "  }\n",
    "  data_param {\n",
    "    source: \"examples/HCCR/HWDB1.1_3755_train_lmdb_linux\"\n",
    "    batch_size: 100 // 每次训练采用的图片64张，min-batch  \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "第一层卷积，权重初始化采用了``gaussian``，其中``std``为 0.118。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer {\n",
    "  type: \"Convolution\" // 卷积层\n",
    "  top: \"Convolution1\"\n",
    "  param {\n",
    "    lr_mult: 1 \n",
    "    decay_mult: 1 \n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2 \n",
    "    decay_mult: 0 \n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 16 // 定义输出特征图个数\n",
    "    pad: 1\n",
    "    kernel_size: 3 // 定义卷积核大小  \n",
    "    stride: 1 \n",
    "    weight_filler {\n",
    "      type: \"gaussian\" // 权重初始化函数\n",
    "      std: 0.118 // 初始化参数\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\" // 偏置类型，应该是默认\n",
    "      value: 0 // 偏置值，应该是默认\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_iter: 100 // 100 * batch_size = 1W 也就是20W测试集每次测试 1 / 20\n",
    "# Carry out testing every 1000 training iterations.\n",
    "test_interval: 1000  // 训练 1000 次测试一次\n",
    "# The base learning rate, momentum and the weight decay of the network.\n",
    "base_lr: 0.1 // 基础学习率，没看到优化函数类型，难道caffe并没有明确优化函数类型\n",
    "momentum: 0.9  // 学习率动量\n",
    "weight_decay: 0.0001 // decay\n",
    "# The learning rate policy\n",
    "lr_policy: \"multistep\" // 学习率策略\n",
    "gamma:0.1\n",
    "stepvalue:40000\n",
    "stepvalue:80000\n",
    "# Display every 200 iterations\n",
    "display: 100  // 100个迭代显示一次结果？那个 test_iter有什么区别？难道是指训练100个batch算一个 epoch？\n",
    "# The maximum number of iterations\n",
    "max_iter: 100000 // 最多训练10W迭代，按照我目前的测试来看，我2000个bathc的epoch差不多10分钟，我的batch设置的是256，换算下来是2500个迭代，看来不可能跑这么久\n",
    "# snapshot intermediate results\n",
    "snapshot: 10000 // 多久保存一次weigth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "后来联系到 github 上这个项目的作者，按他的说法，模型和调参均为仔细调试，就能得到不错的效果，所以这里暂时不再进一步分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### ResNet 的理解及其 Keras 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[博客地址](http://lanbing510.info/2017/08/21/ResNet-Keras.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Input, Activation, Dense, Flatten\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "import six\n",
    "\n",
    "def _handle_dim_ordering():\n",
    "    global ROW_AXIS\n",
    "    global COL_AXIS\n",
    "    global CHANNEL_AXIS\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        ROW_AXIS = 1\n",
    "        COL_AXIS = 2\n",
    "        CHANNEL_AXIS = 3\n",
    "    else:\n",
    "        CHANNEL_AXIS = 1\n",
    "        ROW_AXIS = 2\n",
    "        COL_AXIS = 3\n",
    "\n",
    "def _get_block(identifier):\n",
    "    if isinstance(identifier, six.string_types):\n",
    "        res = globals().get(identifier)\n",
    "        if not res:\n",
    "            raise ValueError('Invalid {}'.format(identifier))\n",
    "        return res\n",
    "    return identifier\n",
    "\n",
    "def _bn_relu(input):\n",
    "    \"\"\"\n",
    "    Helper to build a BN -> relu block\n",
    "    \"\"\"\n",
    "\n",
    "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
    "    return Activation(\"relu\")(norm)\n",
    "\n",
    "def _conv_bn_relu(**conv_params):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper to build a conv -> BN -> relu block\n",
    "    \"\"\"\n",
    "\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        conv = Conv2D(filters=filters, kernel_size=kernel_size,strides=strides, padding=padding,kernel_initializer=kernel_initializer,kernel_regularizer=kernel_regularizer)(input)\n",
    "        return _bn_relu(conv)\n",
    "    return f\n",
    "\n",
    "def _bn_relu_conv(**conv_params):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper to build a BN -> relu -> conv block.\n",
    "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        activation = _bn_relu(input)\n",
    "        return Conv2D(filters=filters, kernel_size=kernel_size,strides=strides, padding=padding, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(activation)\n",
    "    return f\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "\n",
    "    \"\"\"\n",
    "    Adds a shortcut between input and residual block and merges them with \"sum\"\n",
    "    \"\"\"\n",
    "    # Expand channels of shortcut to match residual.\n",
    "    # Stride appropriately to match residual (width, height)\n",
    "    # Should be int if network architecture is correctly configured.\n",
    "\n",
    "    input_shape = K.int_shape(input)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
    "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
    "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
    "\n",
    "    shortcut = input\n",
    "\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS], kernel_size=(1, 1), strides=(stride_width, stride_height), padding=\"valid\", kernel_initializer=\"he_normal\",kernel_regularizer=l2(0.0001))(input)\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Builds a residual block with repeating bottleneck blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def f(input):\n",
    "        for i in range(repetitions):\n",
    "            init_strides = (1, 1)\n",
    "            if i == 0 and not is_first_layer:\n",
    "                init_strides = (2, 2)\n",
    "            input = block_function(filters=filters, init_strides=init_strides, is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
    "        return input\n",
    "    return f\n",
    "\n",
    "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def f(input):\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),strides=init_strides, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3), strides=init_strides)(input)\n",
    "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
    "        return _shortcut(input, residual)\n",
    "    return f\n",
    "\n",
    "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Bottleneck architecture for > 34 layer resnet.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    Returns:\n",
    "        A final conv layer of filters * 4\n",
    "    \"\"\"\n",
    "\n",
    "    def f(input):\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1), strides=init_strides, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1), strides=init_strides)(input)\n",
    "\n",
    "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
    "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
    "        return _shortcut(input, residual)\n",
    "    return f\n",
    "\n",
    "class ResnetBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
    "        \"\"\"\n",
    "        Builds a custom ResNet like architecture.\n",
    "        Args:\n",
    "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
    "            num_outputs: The number of outputs at final softmax layer\n",
    "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.The original paper used basic_block for layers < 50\n",
    "            repetitions: Number of repetitions of various block units.At each block unit, the number of filters are doubled and the input size is halved\n",
    "        Returns:\n",
    "            The keras `Model`.\n",
    "        \"\"\"\n",
    "\n",
    "        _handle_dim_ordering()\n",
    "\n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
    "\n",
    "        # Permute dimension order if necessary\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            input_shape = (input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "        # Load function from str if needed.\n",
    "        block_fn = _get_block(block_fn)\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
    "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
    "\n",
    "        block = pool1\n",
    "        filters = 64\n",
    "        for i, r in enumerate(repetitions):\n",
    "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
    "            filters *= 2\n",
    "\n",
    "        # Last activation\n",
    "        block = _bn_relu(block)\n",
    "\n",
    "        # Classifier block\n",
    "        block_shape = K.int_shape(block)\n",
    "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]), strides=(1, 1))(block)\n",
    "        flatten1 = Flatten()(pool2)\n",
    "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\", activation=\"softmax\")(flatten1)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense)\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_18(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_34(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_50(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_101(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_152(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'Resnet18'\n",
    "model = ResnetBuilder.build_resnet_18((pix, pix, 1), num_class)\n",
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### ResNet TensorFlow 实现参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[github项目地址](https://github.com/xuyuwei/resnet-tfJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 在 jupter 中 import 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "根据博客的内容，ResNet的类已经有了，但是我需要在 jupyter 中 import，文件都在 python_code 目录中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!ls python_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "为了测试，写了一个 jupyter_import_test.py 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!cat python_code/jupyter_import_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from python_code.jupyter_import_test import import_test\n",
    "import_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'Resnet18'\n",
    "param_rate = 1\n",
    "model = ResnetBuilder.build_resnet_18((pix, pix, 1), num_class)\n",
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'Resnet34'\n",
    "param_rate = 1.5\n",
    "model = ResnetBuilder.build_resnet_34((pix, pix, 1), num_class)\n",
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'Resnet50'\n",
    "param_rate = 2\n",
    "model = ResnetBuilder.build_resnet_50((pix, pix, 1), num_class)\n",
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'Resnet101'\n",
    "param_rate = 4\n",
    "model = ResnetBuilder.build_resnet_101((pix, pix, 1), num_class)\n",
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'Resnet152'\n",
    "param_rate = 6\n",
    "model = ResnetBuilder.build_resnet_152((pix, pix, 1), num_class)\n",
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Xception_Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2017年的state-of-the-art图像分类模型，优先尝试，[官方说明](https://keras-cn.readthedocs.io/en/latest/other/application/#xception)\n",
    "从函数说明中可以看出，input_shape如果要修改就必须设置 include_top=False，也就说删除顶层的全连接层，但是如果要设置 classes 又需要这些全连接层，花了不少功夫找到了[解决方法](https://github.com/keras-team/keras/issues/4465)，然而，通道数被限制为3，放弃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'xception_keras'\n",
    "\n",
    "def get_xception_keras_model():\n",
    "    model_xception = Xception(weights=None, include_top=False)\n",
    "    input_layer = Input(shape=(64, 64, 1), name='image_input')\n",
    "    output_model_xception = model_xception(input_layer)\n",
    "    x = Flatten(name='flatten')(output_model_xception)\n",
    "    x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    x = Dense(num_class, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model\n",
    "\n",
    "model = get_xception_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### sigle_dense_v.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "主要用于测试 ModelCheckpoint/TensorBoard 是否生效，测试发现不生效，尝试将数据保存至train下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 定义存储目录\n",
    "# train_dir_name = os.path.join(log_dir, model_dir_name, 'v.01')\n",
    "train_dir_name = os.path.join(log_dir, 'train')\n",
    "\n",
    "# 通用代码，无需更改\n",
    "if os.path.exists(train_dir_name):\n",
    "    os.system('rm -rf %s' % (train_dir_name));\n",
    "print(train_dir_name)\n",
    "check_cb = ModelCheckpoint(filepath=os.path.join(train_dir_name , 'weights.hdf5'), verbose=1, save_best_only=True)\n",
    "board_cb = TensorBoard(log_dir=train_dir_name, histogram_freq=False, write_graph=True, write_images=False)\n",
    "\n",
    "# 开始训练\n",
    "model.fit_generator(train_gen, steps_per_epoch=10, epochs=3, \n",
    "                    validation_data=test_gen, validation_steps=1, \n",
    "                    callbacks=[check_cb, board_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### sigle_dense_v.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "主要用于测试生成的权重文件是否可以使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 定义存储目录\n",
    "train_dir_name = os.path.join(log_dir, model_dir_name, 'v.02')\n",
    "\n",
    "# 通用代码，无需更改\n",
    "if os.path.exists(train_dir_name):\n",
    "    os.system('rm -rf %s' % (train_dir_name));\n",
    "print(train_dir_name)\n",
    "weights_name = os.path.join(train_dir_name , 'weights.hdf5')\n",
    "check_cb = ModelCheckpoint(filepath=weights_name, verbose=1, save_best_only=True)\n",
    "board_cb = TensorBoard(log_dir=train_dir_name, histogram_freq=False, write_graph=False, write_images=False)\n",
    "\n",
    "# 开始训练\n",
    "model.fit_generator(train_gen, steps_per_epoch=num_train//batch_size, epochs=4, \n",
    "                    validation_data=test_gen, validation_steps=num_test//batch_size, \n",
    "                    callbacks=[check_cb, board_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "这里没有搞清楚 model.load_weights(filepath, by_name=False) 中 by_name 的作用，反正用了就预测错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = sigle_dense_model()\n",
    "model.load_weights('/aiml/dfs/checkpoint/sigle_dense/v.02/weights.hdf5')\n",
    "model.evaluate_generator(test_gen)\n",
    "len(model.predict_generator(test_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### sigle_dense_v.03 —— TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "这一步主要用于测试 AMC 平台日志打印功能、Tensorboard、权重文件保存是否正常。模型验证正确，于是进入 AMC 平台进行测试，一个非常细小的失误（文件名后缀写错），但是也许因为排队的人非常多，每次提交task要等非常久才会报错，导致周五、周六两天调试了很多次也没能运行模型。这里有个想法，AMC 平台也许可以在这种比赛期间，专门提供一个用于调试的训练环境，无需排队，每个任务最多执行10分钟，专门用于调测模型是否能够正常运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ResNet_v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "优先使用 MNist 测试集，看该模型是否收敛，确认Res模型正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 定义存储目录\n",
    "train_dir_name = os.path.join(log_dir, model_dir_name, 'v.01')\n",
    "\n",
    "# 通用代码，无需更改\n",
    "if os.path.exists(train_dir_name):\n",
    "    os.system('rm -rf %s' % (train_dir_name));\n",
    "print(train_dir_name)\n",
    "weights_name = os.path.join(train_dir_name , 'weights.hdf5')\n",
    "check_cb = ModelCheckpoint(filepath=weights_name, verbose=1, save_best_only=True)\n",
    "board_cb = TensorBoard(log_dir=train_dir_name, histogram_freq=False, write_graph=False, write_images=False)\n",
    "\n",
    "# 开始训练\n",
    "model.fit_generator(train_gen, steps_per_epoch=num_train//batch_size, epochs=4, \n",
    "                    validation_data=test_gen, validation_steps=num_test//batch_size, \n",
    "                    callbacks=[check_cb, board_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ResNet_v.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "主要测试在 HWDB 集合上的可执行性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 定义存储目录\n",
    "train_dir_name = os.path.join(log_dir, model_dir_name, 'v.02')\n",
    "\n",
    "# 通用代码，无需更改\n",
    "if os.path.exists(train_dir_name):\n",
    "    os.system('rm -rf %s' % (train_dir_name));\n",
    "print(train_dir_name)\n",
    "weights_name = os.path.join(train_dir_name , 'weights.hdf5')\n",
    "check_cb = ModelCheckpoint(filepath=weights_name, verbose=1, save_best_only=True)\n",
    "board_cb = TensorBoard(log_dir=train_dir_name, histogram_freq=False, write_graph=False, write_images=False)\n",
    "\n",
    "# 开始训练\n",
    "model.fit_generator(train_gen, steps_per_epoch=100, epochs=1, \n",
    "                    validation_data=test_gen, validation_steps=10, \n",
    "                    callbacks=[check_cb, board_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ResNet18网络的GPU环境下，在batch_size为512的情况下，每个batch大约需要0.5s。那么可以大约估计Res18/34/50/101/152,每个bantch需要大约0.5s/0.75s/1s/2s/3s,也就是说，一个 Epoch(1000 batch)需要10m/15m/20m/40m/60m,那么跑完80W训练集大致需要20/40/80/160m。运行起来后，再确认该数值，用于调整实测相同配置下：165/173/252s/340/?，152层估计也用不到，而且 batch 512 直接报错，考虑 18、32、50、101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ResNet_v.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "主要分析 3 种网络（除了 152和101）的性能差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 定义存储目录\n",
    "train_dir_name = os.path.join(log_dir, model_dir_name, 'v.02')\n",
    "\n",
    "# 通用代码，无需更改\n",
    "if os.path.exists(train_dir_name):\n",
    "    os.system('rm -rf %s' % (train_dir_name));\n",
    "print(train_dir_name)\n",
    "weights_name = os.path.join(train_dir_name , 'weights.hdf5')\n",
    "check_cb = ModelCheckpoint(filepath=weights_name, verbose=1, save_best_only=True)\n",
    "board_cb = TensorBoard(log_dir=train_dir_name, histogram_freq=False, write_graph=False, write_images=False)\n",
    "step_per_epoch /= param_rate\n",
    "step_per_valid /= param_rate\n",
    "max_epoch /= param_rate\n",
    "print(\"Batch_size: %d\\nStep_per_epoch: %d\\nstep_per_valid: %d\\nMax_epoch: %d\" % (batch_size, step_per_epoch, step_per_valid, max_epoch))\n",
    "\n",
    "# 开始训练\n",
    "model.fit_generator(train_gen, steps_per_epoch=step_per_epoch, epochs=max_epoch, \n",
    "                    validation_data=test_gen, validation_steps=step_per_valid, \n",
    "                    callbacks=[check_cb, board_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Resnet运行4小时，训练样本94%，测试样本88%。向训练过ResNet20的人了解了下，他达到最优只用了2个小时，所以肯定什么地方搞错了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Res152、Res101在batch512的情况下显存不够，直接放弃，剩下几个网络在10小时左右，训练集的准确率都在97%左右徘徊，考虑到模型文件的大小140M/250M\n",
    "/340M，所以决定采用Res18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 优化函数和学习率 —— TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "尝试使用 SGD 优化函数，现在 MNIST 上测试,参考 ResNet20 训练人的数据，lr=0.1, momentum=0.9, decay=0.0001, nesterov=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'Resnet18'\n",
    "model = ResnetBuilder.build_resnet_18((pix, pix, 1), num_class)\n",
    "model.compile(SGD(lr=0.1, momentum=0.8, decay=0.0001, nesterov=True), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 定义存储目录\n",
    "train_dir_name = os.path.join(log_dir, model_dir_name, 'v.03')\n",
    "\n",
    "# 通用代码，无需更改\n",
    "if os.path.exists(train_dir_name):\n",
    "    os.system('rm -rf %s' % (train_dir_name));\n",
    "print(train_dir_name)\n",
    "weights_name = os.path.join(train_dir_name , 'weights.hdf5')\n",
    "check_cb = ModelCheckpoint(filepath=weights_name, verbose=1, save_best_only=True)\n",
    "board_cb = TensorBoard(log_dir=train_dir_name, histogram_freq=False, write_graph=False, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "model.fit_generator(train_gen, steps_per_epoch=num_train//batch_size, epochs=4, \n",
    "                    validation_data=test_gen, validation_steps=num_test//batch_size, \n",
    "                    callbacks=[check_cb, board_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图像预处理  —— TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = image.ImageDataGenerator()\n",
    "test_gen = train_data.flow_from_directory(test_dir, color_mode='grayscale', target_size=(pix, pix), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst(np_pgn):\n",
    "    edges1 = filters.gaussian(np_pgn, sigma=10)\n",
    "    edges2 = exposure.rescale_intensity(edges1)\n",
    "    return edges2\n",
    "test_data = image.ImageDataGenerator(preprocessing_function=tst)\n",
    "test_gen = train_data.flow_from_directory(test_dir, color_mode='grayscale', target_size=(pix, pix), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选N张图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "pixs = test_gen.next()[0][:n].reshape(n, pix, pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(np.min(pixs.flatten()))\n",
    "plots(pixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescale=1./255\n",
    "int(pixs.flatten()[0])\n",
    "plots(pixs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印原始图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots(pixs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma Correction 效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_adjust_gamma = exposure.adjust_gamma(pixs)\n",
    "plots(after_adjust_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_adjust_log = exposure.adjust_log(pixs)\n",
    "plots(after_adjust_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_adjust_sigmoid = exposure.adjust_sigmoid(pixs)\n",
    "plots(after_adjust_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after_histogram = exposure.histogram(pixs[0])\n",
    "#plt.plot(after_histogram[1], after_histogram[0])\n",
    "from skimage import data\n",
    "\n",
    "plot(data.camera())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_rescale_intensity = exposure.rescale_intensity(pixs)\n",
    "plots(after_rescale_intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，深色的部分大部分没有值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png = PIL.Image.open('/aiml/data/train/00001/102681.png')\n",
    "np_pgn = np.array(png)\n",
    "plot(np_pgn)\n",
    "np.bincount(np.array(png).flatten())\n",
    "\n",
    "after_rescale_intensity = exposure.rescale_intensity(np_pgn)\n",
    "plot(after_rescale_intensity)\n",
    "np.bincount(after_rescale_intensity.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage import data,filters\n",
    "img=io.imread('/aiml/data/train/00001/102681.png',as_grey=True)\n",
    "img.shape\n",
    "np_pgn = np.ndarray(img.shape, buffer=img)\n",
    "\n",
    "np_pgn = np.ndarray(np_pgn.shape, buffer=np_pgn)\n",
    "np_pgn1 = exposure.rescale_intensity(np_pgn)\n",
    "\n",
    "edges1 = filters.gaussian(np_pgn, sigma=2)\n",
    "edges2 = exposure.rescale_intensity(edges1)\n",
    "edges5 = exposure.rescale_intensity(np_pgn)\n",
    "edges6 = filters.gaussian(edges5, sigma=2)\n",
    "edges7 = exposure.rescale_intensity(edges6)\n",
    "\n",
    "edges3 = filters.median(np_pgn, disk(4))\n",
    "edges4 = exposure.rescale_intensity(edges3)\n",
    "\n",
    "plots([np_pgn, np_pgn1])\n",
    "plots([edges1, edges2, edges5, edges6, edges7])\n",
    "plots([edges3, edges4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage import data,filters\n",
    "img=io.imread('/aiml/data/train/00001/102681.png',as_grey=True)\n",
    "img.shape\n",
    "np_pgn = np.ndarray(img.shape, buffer=img)\n",
    "\n",
    "\n",
    "edges = filters.sobel(np_pgn)\n",
    "edges1 = filters.scharr(np_pgn)\n",
    "edges2 = filters.roberts(np_pgn)\n",
    "edges3 = filters.prewitt(np_pgn)\n",
    "plots([edges, edges1, edges2, edges3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_pgn = np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "膨胀，1~4之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(sm.dilation(tmp[2].reshape(64, 64),sm.square(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "腐蚀，1~4之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(sm.erosion(tmp[2].reshape(64, 64),sm.square(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 其他尝试  —— TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ReduceLROnPlateau 试用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* http://yangguang2009.github.io/2017/01/08/deeplearning/grid-search-hyperparameters-for-deep-learning/\n",
    "* https://sherlockliao.github.io/2017/11/26/%E5%A6%82%E4%BD%95%E6%89%BE%E5%88%B0%E6%9C%80%E5%A5%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87/\n",
    "* https://www.jianshu.com/p/d8222a84613c\n",
    "* http://yufeigan.github.io/2014/11/29/Deep-Learning-%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/\n",
    "* https://github.com/xuyuwei/resnet-tf\n",
    "* http://lanbing510.info/2017/08/21/ResNet-Keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 探索学习率衰减规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "尝试打印 lr=0.1, momentum=0.9, decay=0.0001, nesterov=False配置下，每轮epoch后的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_data = image.ImageDataGenerator(rescale=1./255)\n",
    "test_gen = train_data.flow_from_directory(test_dir, color_mode='grayscale', target_size=(pix, pix), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 绘制Res50结构图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "需要安装以下库，才能正常绘制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install pydot\n",
    "pip install pydot_ng\n",
    "apt-get install -y graphviz libgraphviz-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 这个是自己实现的 Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='ResNet50.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![title](ResNet50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Keras 官方实现的Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ResNet50?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = ResNet50(weights=None)\n",
    "plot_model(model, 'ResNet50_offical.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![title](ResNet50_offical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#####  对比  Resnet18 和 Resnet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = ResnetBuilder.build_resnet_18((pix, pix, 1), num_class)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_resnet((pix, pix, 1), num_class)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 打印model中间层输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_lin_model():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape = (28, 28, 1)),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = get_lin_model()\n",
    "np.set_printoptions(suppress=True, linewidth=240)\n",
    "intermediate_tensor_function = K.function([model.layers[0].input],[model.layers[0].output])\n",
    "intermediate_tensor = intermediate_tensor_function([x_train[1:2]])[0]\n",
    "intermediate_tensor.reshape(28, 28).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "intermediate_tensor_function = K.function([model.layers[0].input],[model.layers[1].output])\n",
    "intermediate_tensor_function([x_train[1:2]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.array(model.get_weights()[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 重新读取gnt文件，预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %load /tmp/tmp.py\n",
    "import os\n",
    "import numpy as np\n",
    "import struct\n",
    "import PIL.Image\n",
    "import scipy\n",
    "\n",
    "train_data_dir = '/aiml/data/HWDB1.1trn_gnt/'\n",
    "test_data_dir = '/aiml/data/HWDB1.1trn_gnt/'\n",
    "log_dir = '/aiml/dfs/checkpoint/train/'\n",
    "\n",
    "# 读取图像和对应的汉字\n",
    "def read_from_gnt_dir(gnt_dir=train_data_dir):\n",
    "    def one_file(f):\n",
    "        header_size = 10\n",
    "        while True:\n",
    "            header = np.fromfile(f, dtype='uint8', count=header_size)\n",
    "            if not header.size: break\n",
    "            sample_size = header[0] + (header[1]<<8) + (header[2]<<16) + (header[3]<<24)\n",
    "            tagcode = header[5] + (header[4]<<8)\n",
    "            width = header[6] + (header[7]<<8)\n",
    "            height = header[8] + (header[9]<<8)\n",
    "            if header_size + width*height != sample_size:\n",
    "                break\n",
    "            image = np.fromfile(f, dtype='uint8', count=width*height).reshape((height, width))\n",
    "            yield image, tagcode\n",
    "\n",
    "    for file_name in os.listdir(gnt_dir):\n",
    "        if file_name.endswith('.gnt'):\n",
    "            file_path = os.path.join(gnt_dir, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                for image, tagcode in one_file(f):\n",
    "                    yield image, tagcode\n",
    "\n",
    "def resize_and_normalize_image(img):\n",
    "    # 补方\n",
    "    pad_size = abs(img.shape[0]-img.shape[1]) // 2\n",
    "    if img.shape[0] < img.shape[1]:\n",
    "        pad_dims = ((pad_size, pad_size), (0, 0))\n",
    "    else:\n",
    "        pad_dims = ((0, 0), (pad_size, pad_size))\n",
    "        img = np.lib.pad(img, pad_dims, mode='constant', constant_values=255)\n",
    "        # 缩放\n",
    "        img = scipy.misc.imresize(img, (64 - 4*2, 64 - 4*2))\n",
    "        img = np.lib.pad(img, ((4, 4), (4, 4)), mode='constant', constant_values=255)\n",
    "        assert img.shape == (64, 64)\n",
    "\n",
    "    #img = img.flatten()\n",
    "    # 像素值范围-1到1\n",
    "    #img = (img - 128) / 128\n",
    "    return img\n",
    "\n",
    "# one hot\n",
    "def convert_to_one_hot(char):\n",
    "    vector = np.zeros(len(char_set))\n",
    "    vector[char_set.index(char)] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen = read_from_gnt_dir(gnt_dir=train_data_dir)\n",
    "tmp = gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp[0].shape\n",
    "print(struct.pack('>H', tmp[1]).decode('gb2312'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "resize_and_normalize_image(tmp[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot(resize_and_normalize_image(tmp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "将图片读入内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_data_x = []\n",
    "train_data_y = []\n",
    "for image, tagcode in read_from_gnt_dir(gnt_dir=train_data_dir):\n",
    "    tagcode_unicode = struct.pack('>H', tagcode).decode('gb2312')\n",
    "    train_data_x.append(resize_and_normalize_image(image))\n",
    "    train_data_y.append(convert_to_one_hot(tagcode_unicode))\n",
    "\n",
    "test_data_x = []\n",
    "test_data_y = []\n",
    "for image, tagcode in read_from_gnt_dir(gnt_dir=test_data_dir):\n",
    "    tagcode_unicode = struct.pack('>H', tagcode).decode('gb2312')\n",
    "    text_data_x.append(resize_and_normalize_image(image))\n",
    "    text_data_y.append(convert_to_one_hot(tagcode_unicode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "生成字典文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "char_set = set()\n",
    "for _, tagcode in read_from_gnt_dir(gnt_dir=train_data_dir):\n",
    "    tagcode_unicode = struct.pack('>H', tagcode).decode('gb2312')\n",
    "    char_set.add(tagcode_unicode)\n",
    "char_list = list(char_set)\n",
    "char_dict = dict(zip(sorted(char_list), range(len(char_list))))\n",
    "print len(char_dict)\n",
    "import pickle\n",
    "f = open('char_dict', 'wb')\n",
    "pickle.dump(char_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "将文件保存至硬盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_counter = 0\n",
    "test_counter = 0\n",
    "for image, tagcode in read_from_gnt_dir(gnt_dir=train_data_dir):\n",
    "    tagcode_unicode = struct.pack('>H', tagcode).decode('gb2312')\n",
    "    # TODO\n",
    "    print((image.flatten() == tmp[0].flatten()).all())\n",
    "    im = PIL.Image.fromarray(image)\n",
    "    dir_name = '/tmp/data/train/' + '%0.5d'%char_dict[tagcode_unicode]\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    im.convert('L').save(dir_name+'/' + str(train_counter) + '.png')\n",
    "    train_counter += 1\n",
    "    break\n",
    "\n",
    "for image, tagcode in read_from_gnt_dir(gnt_dir=test_data_dir):\n",
    "    tagcode_unicode = struct.pack('>H', tagcode).decode('gb2312')\n",
    "    im = PIL.Image.fromarray(image)\n",
    "    dir_name = '/tmp/data/test/' + '%0.5d'%char_dict[tagcode_unicode]\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    im.convert('L').save(dir_name+'/' + str(test_counter) + '.png')\n",
    "    test_counter += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "im.convert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "验证写入硬盘的文件没有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "png = PIL.Image.open('/tmp/data/train/03051/0.png', )\n",
    "np_pgn = np.array(png)\n",
    "plot(np_pgn)\n",
    "np.bincount(np_pgn.flatten())\n",
    "np.bincount(tmp[0].flatten())\n",
    "np_pgn.shape\n",
    "(np_pgn.flatten() == tmp[0].flatten()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 调用 C++ 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from ctypes import *\n",
    "import struct\n",
    "import numpy as np\n",
    "from numpy.ctypeslib import ndpointer\n",
    "\n",
    "def resize_with_c(origin_img, width, height):\n",
    "    # 转化为 ctyes 类型的输入\n",
    "    origin_img = np.array(origin_img).astype(np.uint16)\n",
    "    origin_img_ctype = (c_ubyte * len(origin_img))()\n",
    "    for i in np.arange(len(origin_img)):\n",
    "        origin_img_ctype[i]= int(origin_img[i])\n",
    "\n",
    "    # 调用 C++ 接口处理\n",
    "    pdll = CDLL('/root/jupyter/HCCR-ResNet/convert_HWDB_dir/my8.so')\n",
    "    pdll.process.argtypes = [c_ubyte * len(origin_img), c_ushort, c_ushort]\n",
    "    pdll.process.restype = ndpointer(dtype=c_int, shape=(64, 64))\n",
    "    resize_img = pdll.process(origin_img_ctype, width, height)\n",
    "    \n",
    "    # 将输出转化为 np.array\n",
    "    return resize_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy.ctypeslib import ndpointer\n",
    "from ctypes import *\n",
    "\n",
    "lib = CDLL('/root/jupyter/HCCR-ResNet/convert_HWDB_dir/my1.so')\n",
    "\n",
    "lib.function.restype = ndpointer(dtype=c_ubyte, shape=(10,))\n",
    "\n",
    "res = lib.function()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f = open('/root/jupyter/HCCR-ResNet/convert_HWDB_dir/Image1.png', 'rb')\n",
    "test = f.read()\n",
    "origin_img = struct.unpack('B' * 50 * 92, test)\n",
    "\n",
    "np.max(np.array(origin_img).astype(np.uint16)), np.min(np.array(origin_img).astype(np.uint16))\n",
    "\n",
    "resize_img = resize_with_c(origin_img, 50, 92)\n",
    "\n",
    "plot(np.array(origin_img).reshape(92, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f = open('/root/jupyter/HCCR-ResNet/convert_HWDB_dir/Image2.png', 'rb')\n",
    "test = f.read()\n",
    "resize_img1 = struct.unpack('B' * 64 * 64, test)\n",
    "(np.array(resize_img1) == resize_img.flatten()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot(resize_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot(np.array(tmp1).reshape(64, 64))\n",
    "np.bincount(np.array(tmp1))\n",
    "(tmp1 == tmp[0].flatten()).all()\n",
    "plot(np.array(tmp1).reshape(92, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 对比三种 Resize 的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import struct\n",
    "import PIL.Image\n",
    "import scipy\n",
    "\n",
    "train_data_dir = '/aiml/data/HWDB1.1trn_gnt/'\n",
    "\n",
    "# 读取图像和对应的汉字\n",
    "def read_from_gnt_dir(gnt_dir=train_data_dir):\n",
    "    def one_file(f):\n",
    "        header_size = 10\n",
    "        while True:\n",
    "            header = np.fromfile(f, dtype='uint8', count=header_size)\n",
    "            if not header.size: break\n",
    "            sample_size = header[0] + (header[1]<<8) + (header[2]<<16) + (header[3]<<24)\n",
    "            tagcode = header[5] + (header[4]<<8)\n",
    "            width = header[6] + (header[7]<<8)\n",
    "            height = header[8] + (header[9]<<8)\n",
    "            if header_size + width*height != sample_size:\n",
    "                break\n",
    "            image = np.fromfile(f, dtype='uint8', count=width*height).reshape((height, width))\n",
    "            yield image, tagcode\n",
    "\n",
    "    for file_name in os.listdir(gnt_dir):\n",
    "        if file_name.endswith('.gnt'):\n",
    "            file_path = os.path.join(gnt_dir, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                for image, tagcode in one_file(f):\n",
    "                    yield image, tagcode\n",
    "                    \n",
    "train_data_x = []\n",
    "train_data_y = []\n",
    "count = 0\n",
    "for image, tagcode in read_from_gnt_dir(gnt_dir=train_data_dir):\n",
    "    tagcode_unicode = struct.pack('>H', tagcode).decode('gb2312')\n",
    "    train_data_x.append(image)\n",
    "    train_data_y.append(tagcode_unicode)\n",
    "    count += 1\n",
    "    if count > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_x = train_data_x[:10]\n",
    "data_y = train_data_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_y)):\n",
    "    plot(data_x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Keras提供的 Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage import exposure, filters, io\n",
    "train_dir = '/tmp/data/train/'\n",
    "os.system(\"rm -rf /tmp/data/train/\")\n",
    "char_dict = {}\n",
    "for y in data_y:\n",
    "    if y not in char_dict:\n",
    "        char_dict[y] = len(char_dict)\n",
    "count = len(data_x)\n",
    "for index in np.arange(count):\n",
    "    image = data_x[index]\n",
    "    tagcode_unicode = data_y[index]\n",
    "    im = PIL.Image.fromarray(image)\n",
    "    dir_name = '/tmp/data/train/' + '%0.5d'%char_dict[tagcode_unicode]\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    im.convert('L').save(dir_name+'/' + str(index) + '.png')\n",
    "\n",
    "pix = 56\n",
    "batch_size = len(data_y)\n",
    "train_data = ImageDataGenerator(preprocessing_function=exposure.rescale_intensity)\n",
    "train_gen = train_data.flow_from_directory(train_dir, \n",
    "                                           color_mode='grayscale', \n",
    "                                           target_size=(pix, pix), \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp1 = train_gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp1 = tmp1[0].reshape(10, pix, pix)\n",
    "for i in np.arange(len(data_y)):\n",
    "    plot(tmp1[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### C++ 实现版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from ctypes import *\n",
    "import struct\n",
    "import numpy as np\n",
    "from numpy.ctypeslib import ndpointer\n",
    "\n",
    "def resize_with_c(origin_img):\n",
    "    # 转化为 ctyes 类型的输入\n",
    "    height, width = origin_img.shape\n",
    "    origin_img = origin_img.flatten()\n",
    "    origin_img = np.array(origin_img).astype(np.uint16)\n",
    "    origin_img_ctype = (c_ubyte * len(origin_img))()\n",
    "    for i in np.arange(len(origin_img)):\n",
    "        origin_img_ctype[i]= int(origin_img[i])\n",
    "\n",
    "    # 调用 C++ 接口处理\n",
    "    pdll = CDLL('/root/jupyter/HCCR-ResNet/convert_HWDB_dir/my8.so')\n",
    "    pdll.process.argtypes = [c_ubyte * len(origin_img), c_ushort, c_ushort]\n",
    "    pdll.process.restype = ndpointer(dtype=c_int, shape=(64, 64))\n",
    "    resize_img = pdll.process(origin_img_ctype, width, height)\n",
    "    \n",
    "    # 将输出转化为 np.array\n",
    "    return resize_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_y)):\n",
    "    plot(resize_with_c(data_x[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### python实现版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def resize_and_normalize_image(img):\n",
    "    # 补方\n",
    "    pad_size = abs(img.shape[0]-img.shape[1]) // 2\n",
    "    if img.shape[0] < img.shape[1]:\n",
    "        pad_dims = ((pad_size, pad_size), (0, 0))\n",
    "    else:\n",
    "        pad_dims = ((0, 0), (pad_size, pad_size))\n",
    "        img = np.lib.pad(img, pad_dims, mode='constant', constant_values=255)\n",
    "        # 缩放\n",
    "        img = scipy.misc.imresize(img, (64 - 4*2, 64 - 4*2))\n",
    "        img = np.lib.pad(img, ((4, 4), (4, 4)), mode='constant', constant_values=255)\n",
    "        assert img.shape == (64, 64)\n",
    "\n",
    "    #img = img.flatten()\n",
    "    # 像素值范围-1到1\n",
    "    #img = (img - 128) / 128\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_y)):\n",
    "    plot(resize_and_normalize_image(data_x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot(resize_and_normalize_image(data_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " plot(resize_with_c(data_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot(tmp1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征标准化——TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算整个图集的标准差和均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[三种常用的归一化](http://www.voidcn.com/article/p-yigvmzkm-gm.html)\n",
    "[Resnet-152的图像预处理](https://zhuanlan.zhihu.com/p/29888789)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {
    "height": "964px",
    "left": "0px",
    "right": "1646px",
    "top": "50px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
