{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#读取-HWDB-数据\" data-toc-modified-id=\"读取-HWDB-数据-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>读取 HWDB 数据</a></span><ul class=\"toc-item\"><li><span><a href=\"#真实训练数据\" data-toc-modified-id=\"真实训练数据-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>真实训练数据</a></span></li><li><span><a href=\"#测试数据-MNIST\" data-toc-modified-id=\"测试数据-MNIST-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>测试数据 MNIST</a></span><ul class=\"toc-item\"><li><span><a href=\"#将-gz-格式的-mnist-数据转换为-png-格式\" data-toc-modified-id=\"将-gz-格式的-mnist-数据转换为-png-格式-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>将 gz 格式的 mnist 数据转换为 png 格式</a></span></li></ul></li></ul></li><li><span><a href=\"#数据预处理\" data-toc-modified-id=\"数据预处理-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>数据预处理</a></span></li><li><span><a href=\"#模型定义\" data-toc-modified-id=\"模型定义-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>模型定义</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sigle-Dense\" data-toc-modified-id=\"Sigle-Dense-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Sigle Dense</a></span></li><li><span><a href=\"#ResNet\" data-toc-modified-id=\"ResNet-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>ResNet</a></span></li></ul></li><li><span><a href=\"#训练\" data-toc-modified-id=\"训练-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>训练</a></span><ul class=\"toc-item\"><li><span><a href=\"#初始化模型\" data-toc-modified-id=\"初始化模型-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>初始化模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#sigle_dense_v.01\" data-toc-modified-id=\"sigle_dense_v.01-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>sigle_dense_v.01</a></span></li></ul></li><li><span><a href=\"#设置每个-epoch-的回调\" data-toc-modified-id=\"设置每个-epoch-的回调-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>设置每个 epoch 的回调</a></span></li><li><span><a href=\"#Sigle-Dense\" data-toc-modified-id=\"Sigle-Dense-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Sigle Dense</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.optimizers import Nadam, Adam, SGD\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取 HWDB 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 真实训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_dir = '/aiml/data/train/'\n",
    "test_dir = '/aiml/data/test/'\n",
    "log_dir = '/aiml/dfs/checkpoint/'\n",
    "pix = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试数据 MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要用于确定数据输入正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = '/aiml/data/mnist_train'\n",
    "test_dir = '/aiml/data/mnist_test'\n",
    "log_dir = '/aiml/dfs/checkpoint/'\n",
    "pix = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 将 gz 格式的 mnist 数据转换为 png 格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "下载并解压 mnist 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/aiml/data/HWDB/10_train/’: File exists\n",
      "mkdir: cannot create directory ‘/aiml/data/HWDB/10_test/’: File exists\n",
      "ls: write error: Broken pipe\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# 获取脚本 https://github.com/myleott/mnist_png\n",
    "!wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "!wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "!zcat train-images-idx3-ubyte.gz > train-images-idx3-ubyte\n",
    "!zcat train-labels-idx1-ubyte.gz > train-labels-idx1-ubyte\n",
    "!pip install pypng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "解压有问题，所以需要将训练集分成两部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for num in [str(x) for x in range(10)]:\n",
    "    train_dir = os.path.join('training', num)\n",
    "    test_dir = os.path.join('testing', num)\n",
    "    train_files = [os.path.join(train_dir, x) for x in os.listdir(train_dir)]\n",
    "    test_files = [os.path.join(test_dir, x) for x in os.listdir(test_dir)]\n",
    "    train_files.sort()\n",
    "    test_files.sort()\n",
    "    [os.remove(x) for x in train_files[:500]]\n",
    "    [os.remove(x) for x in test_files[500:]]\n",
    "!find . -name \"*.png\"|cut -d '/' -f 4 |sort |uniq |wc -l\n",
    "!find . -name \"*.png\"| wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55000 images belonging to 10 classes.\n",
      "Found 5000 images belonging to 10 classes.\n",
      "categorical\n",
      "10 55000 5000 (512, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = image.ImageDataGenerator(samplewise_std_normalization=True)\n",
    "test_data = image.ImageDataGenerator(samplewise_std_normalization=True)\n",
    "train_gen = train_data.flow_from_directory(train_dir, color_mode='grayscale', target_size=(pix, pix), batch_size=batch_size)\n",
    "test_gen = train_data.flow_from_directory(test_dir, color_mode='grayscale', target_size=(pix, pix), batch_size=batch_size)\n",
    "# 类别数量\n",
    "num_class = train_gen.num_classes\n",
    "# 训练样本数量\n",
    "num_train = train_gen.n\n",
    "# 测试样本数量\n",
    "num_test = test_gen.n\n",
    "# 确认读取的图片的格式正确\n",
    "image_shape = train_gen.next()[0].shape\n",
    "# 查看返回的标签类型，“categorical” 表示为 one-hot 标签\n",
    "print(train_gen.class_mode)\n",
    "print(num_class, num_train, num_test, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigle Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要用于测试数据输入正确性，发现loss俨然有上升趋势，放弃其他尝试，换上 MNIST 后，发现一切正常，说明 HCCR 需要的模型较为复杂，收敛时间更长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'sigle_dense'\n",
    "def sigle_dense_model():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(pix, pix, 1)),\n",
    "        Dense(512, activation='softmax'),\n",
    "        Dense(num_class, activation='softmax')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 64s 637ms/step - loss: 0.7836 - acc: 0.9142 - val_loss: 3.2922 - val_acc: 0.5737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fee504eb668>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Input, Activation, Dense, Flatten\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "import six\n",
    "\n",
    "def _handle_dim_ordering():\n",
    "    global ROW_AXIS\n",
    "    global COL_AXIS\n",
    "    global CHANNEL_AXIS\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        ROW_AXIS = 1\n",
    "        COL_AXIS = 2\n",
    "        CHANNEL_AXIS = 3\n",
    "    else:\n",
    "        CHANNEL_AXIS = 1\n",
    "        ROW_AXIS = 2\n",
    "        COL_AXIS = 3\n",
    "\n",
    "def _get_block(identifier):\n",
    "    if isinstance(identifier, six.string_types):\n",
    "        res = globals().get(identifier)\n",
    "        if not res:\n",
    "            raise ValueError('Invalid {}'.format(identifier))\n",
    "        return res\n",
    "    return identifier\n",
    "\n",
    "def _bn_relu(input):\n",
    "    \"\"\"\n",
    "    Helper to build a BN -> relu block\n",
    "    \"\"\"\n",
    "\n",
    "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
    "    return Activation(\"relu\")(norm)\n",
    "\n",
    "def _conv_bn_relu(**conv_params):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper to build a conv -> BN -> relu block\n",
    "    \"\"\"\n",
    "\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        conv = Conv2D(filters=filters, kernel_size=kernel_size,strides=strides, padding=padding,kernel_initializer=kernel_initializer,kernel_regularizer=kernel_regularizer)(input)\n",
    "        return _bn_relu(conv)\n",
    "    return f\n",
    "\n",
    "def _bn_relu_conv(**conv_params):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper to build a BN -> relu -> conv block.\n",
    "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    filters = conv_params[\"filters\"]\n",
    "    kernel_size = conv_params[\"kernel_size\"]\n",
    "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
    "\n",
    "    def f(input):\n",
    "        activation = _bn_relu(input)\n",
    "        return Conv2D(filters=filters, kernel_size=kernel_size,strides=strides, padding=padding, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(activation)\n",
    "    return f\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "\n",
    "    \"\"\"\n",
    "    Adds a shortcut between input and residual block and merges them with \"sum\"\n",
    "    \"\"\"\n",
    "    # Expand channels of shortcut to match residual.\n",
    "    # Stride appropriately to match residual (width, height)\n",
    "    # Should be int if network architecture is correctly configured.\n",
    "\n",
    "    input_shape = K.int_shape(input)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
    "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
    "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
    "\n",
    "    shortcut = input\n",
    "\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS], kernel_size=(1, 1), strides=(stride_width, stride_height), padding=\"valid\", kernel_initializer=\"he_normal\",kernel_regularizer=l2(0.0001))(input)\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Builds a residual block with repeating bottleneck blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def f(input):\n",
    "        for i in range(repetitions):\n",
    "            init_strides = (1, 1)\n",
    "            if i == 0 and not is_first_layer:\n",
    "                init_strides = (2, 2)\n",
    "            input = block_function(filters=filters, init_strides=init_strides, is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
    "        return input\n",
    "    return f\n",
    "\n",
    "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def f(input):\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),strides=init_strides, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3), strides=init_strides)(input)\n",
    "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
    "        return _shortcut(input, residual)\n",
    "    return f\n",
    "\n",
    "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Bottleneck architecture for > 34 layer resnet.\n",
    "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
    "    Returns:\n",
    "        A final conv layer of filters * 4\n",
    "    \"\"\"\n",
    "\n",
    "    def f(input):\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1), strides=init_strides, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(1e-4))(input)\n",
    "        else:\n",
    "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1), strides=init_strides)(input)\n",
    "\n",
    "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
    "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
    "        return _shortcut(input, residual)\n",
    "    return f\n",
    "\n",
    "class ResnetBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
    "        \"\"\"\n",
    "        Builds a custom ResNet like architecture.\n",
    "        Args:\n",
    "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
    "            num_outputs: The number of outputs at final softmax layer\n",
    "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.The original paper used basic_block for layers < 50\n",
    "            repetitions: Number of repetitions of various block units.At each block unit, the number of filters are doubled and the input size is halved\n",
    "        Returns:\n",
    "            The keras `Model`.\n",
    "        \"\"\"\n",
    "\n",
    "        _handle_dim_ordering()\n",
    "\n",
    "        if len(input_shape) != 3:\n",
    "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
    "\n",
    "        # Permute dimension order if necessary\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            input_shape = (input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "        # Load function from str if needed.\n",
    "        block_fn = _get_block(block_fn)\n",
    "\n",
    "        input = Input(shape=input_shape)\n",
    "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
    "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
    "\n",
    "        block = pool1\n",
    "        filters = 64\n",
    "        for i, r in enumerate(repetitions):\n",
    "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
    "            filters *= 2\n",
    "\n",
    "        # Last activation\n",
    "        block = _bn_relu(block)\n",
    "\n",
    "        # Classifier block\n",
    "        block_shape = K.int_shape(block)\n",
    "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]), strides=(1, 1))(block)\n",
    "        flatten1 = Flatten()(pool2)\n",
    "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\", activation=\"softmax\")(flatten1)\n",
    "\n",
    "        model = Model(inputs=input, outputs=dense)\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_18(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_34(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_50(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_101(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_resnet_152(input_shape, num_outputs):\n",
    "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = ResnetBuilder.build_resnet_50((pix, pix, 1), num_class)\n",
    "\n",
    "# model_50.load_weights(\"/aiml/dfs/checkpoint/weights.hdf5\")\n",
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sigle_dense_v.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要用于测试 ModelCheckpoint/TensorBoard 是否生效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_name = os.path.join(log_dir, model_dir_name, 'v.01')\n",
    "if os.path.exists(train_dir_name):\n",
    "    os.removedirs(train_dir_name)\n",
    "model = sigle_dense_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置每个 epoch 的回调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_cb = ModelCheckpoint(filepath=os.path.join(train_dir_name , 'weights.hdf5'), verbose=1, save_best_only=True)\n",
    "board_cb = TensorBoard(log_dir=train_dir_name, histogram_freq=0, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigle Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "106/107 [============================>.] - ETA: 0s - loss: 2.1340 - acc: 0.8051Epoch 00001: val_loss improved from inf to 1.99779, saving model to /aiml/dfs/checkpoint/sigle_dense/v.01/weights.hdf5\n",
      "107/107 [==============================] - 32s 295ms/step - loss: 2.1327 - acc: 0.8060 - val_loss: 1.9978 - val_acc: 0.8978\n",
      "Epoch 2/4\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.8723 - acc: 0.9220Epoch 00002: val_loss improved from 1.99779 to 1.76864, saving model to /aiml/dfs/checkpoint/sigle_dense/v.01/weights.hdf5\n",
      "107/107 [==============================] - 30s 281ms/step - loss: 1.8713 - acc: 0.9220 - val_loss: 1.7686 - val_acc: 0.9204\n",
      "Epoch 3/4\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.6648 - acc: 0.9341Epoch 00003: val_loss improved from 1.76864 to 1.58506, saving model to /aiml/dfs/checkpoint/sigle_dense/v.01/weights.hdf5\n",
      "107/107 [==============================] - 31s 287ms/step - loss: 1.6639 - acc: 0.9340 - val_loss: 1.5851 - val_acc: 0.9256\n",
      "Epoch 4/4\n",
      " 60/107 [===============>..............] - ETA: 11s - loss: 1.5236 - acc: 0.9398"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_gen, steps_per_epoch=num_train//batch_size, epochs=4, validation_data=test_gen, validation_steps=num_test//batch_size, callbacks=[check_cb, board_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
