---
title: 利用深度学习探索代码生成任务
date: 2017-11-23 22:33:35
categories: 机器学习
tags:
  - NLP
---

<!-- 文章开头都用它了，整齐才好看 -->
{% note default %}
最近的工作是探索代码生成，记录一些想法
{% endnote %}

<!--more-->

---

# 写在前面
&#8195;&#8195; 最近一段时间主要投入利用深度学习做代码生成的工作，经过一段时间的调研逐步的认识到这个问题的困难，不管后续进展如何，总结一下心得也算是个交代。
&#8195;&#8195; 以我浅薄的数学和机器学习相关的知识来看，代码生成问题很可能并不适合使用深度学习来完成。深度学习更适合处理连续空间的问题，因为深度学习中最基本的一个工具：梯度下降，就是假设整个问题就是一个连续的空间。另外，损失函数要求是可微的，可微的就意味着是连续的。由此，我简单的猜测，深度学习更适合图像类的连续问题，那么，为什么在翻译任务中深度学习表现的也那么好呢？我想那是因为翻译任务更加接近优化任务，输出序列存在较少的约束。
&#8195;&#8195; 回过头来看看代码生成问题，代码生成是一个有着较多约束的问题。首先，生成的程序如果要能编译通过，就一定要符合语法，而语法就是约束。想来翻译任务也能翻译出符合自然语言语法的内容啊，但是，翻译的结果对语法的要求并不那么严格，同时，模型本身也并不需要学习到诸如 ``左右括号必须匹配`` 这样的问题。翻译模型只需要知道 ``I am Chinese`` 序列解码后的第一个词很可能是 ``我``，然后在 ``I am Chinese`` 的前提下 ``我`` 后面有很大概率是 ``是``，以此类推。可以看出翻译模型并不知道约束，只知道在 ‘上下文’ 环境里，下次一个词最有可能是什么就够了，换句话说，翻译模型是在一个完整的 ``token 空间`` 内找到那个最可能出现的词。所以，在不存在约束的情况下，我们可以简单的认为，这也是一个连续空间内的搜索的问题。那么，代码生成问题呢？显示不是，我生成了第一个关键字 `sys`，无论是概率角度还是语法角度，后一个输出都很可能是 ``.``，但是，第三个输出呢？还是在完整的 ``token 空间`` 内搜索么？不是的，``sys`` 是个 module, 也就是说第三个输出是 ``token 空间`` 中的一个很小的子集。按我的理解，数学上最容易表达约束的就是分段函数，一旦分段，就没有连续性，梯度下降在某个点的偏导就可能是未定义的。
&#8195; &#8195; 抱怨困难或者找借口总是简单的，然而却毫无意义，拿着公司的钱只能是硬着头皮上。后续我会继续分析相关的几个 Demo，例如：NMT、Pix2Code、DeepCoder 等模型。

#
代码生成可行性分析
```
    搜索空间大小
    Translate = 40000 ^ 50 = 1.26 e+ 230
    pix2code =15 ^ 60 = 3.68 e+ 70
    case2script = 20000 ^ 200 = 1.60 e+ 860
        仅预测 AW 接口 1000 ^ 25 = 1. e+ 75
    目标函数复杂度
        输入输出差异性
            Translate：自然语言 -> 自然语言
            pix2code：图像 -> 代码
            case2script：自然语言 -> 代码
                case2script = pix2code >Translate
                pix2code 使用 CNN 捕获图像特征，根据特征进行代码生成，case2script 需要自然语言理解，根据语义进行代码生成，传统的翻译模型是否具有自然语言理解的能力？
        输出输出信息量差异
            Translate：信息量一致
            pix2code：信息量一致
            case2script：输入信息量小于输出信息量
            case2script > pix2code = Translate
            参考翻译模型：如何由“意图”推理出“实现”？——知识图谱？
            参考 pix2code：如何抽取特征 / 噪声过滤？——有注意力机制的 RNN？CNN 的特性就是抽取图像特性，RNN 的特性是获取时间序列特性是否合适？
    可用训练集大小
        Translate = 2000W
        pix2code = 1750
        case2script = 10W / 4 = 2W
        数据质量：pix2code > Translate  > case2script 如何清洗数据？
    训练目标
        Translate：勉强可理解
        pix2code：77% DSL
        case2script：100% GIL
        case2script > pix2code > Translate
        DSL 相比于 GIL，除了搜索空间较小以外，很重要的一点是绕开了“复杂结构生成“问题，例如，函数名和参数名预测正确的情况下，还要保证函数（参数 1，参数 2）这样的结构，而且无需考虑括号匹配、分支等问题
```
